defaults:
  - data: default
  - trainer: llasmol_mistral_7b

# General settings
filename: llasmol_test
seed: 42
mode: test

wandb_entity: jennyshin_gist_2024
wandb_project: mol-llm-llasmol
wandb_log_freq: 100
wandb_id: null

debug: true
ckpt_path: null
pretrained_ckpt_path: null
logging_dir: /workspace/Mol_DA/Mol-LLM-Benchmark/checkpoint/HJ_1223/logs/intrinsic_false/llasmol_20251223
# LLM 설정 (LLaSMol-Mistral-7B)
# Base model + LoRA adapter
llm_model: mistralai/Mistral-7B-v0.1
peft_dir: osunlp/LlaSMol-Mistral-7B

# Benchmark mode - use pure LLM without multimodal components
benchmark: true

# LoRA 모드로 설정 (adapter 로드)
tune_llm: lora

# Molecule representation - Text only (no graph)
mol_representation: string_only
num_query_token: 0  # Q-Former도 사용 안함

shuffle_selfies: false
shuffle_graph: false
process_disjoint: true

include_failed_as_zero: true

# Prompt 설정
# intrinsic_prompt: true → ChemDFM 맞춤 프롬프트 사용 (기본값)
# intrinsic_prompt: false → 원본 프롬프트 유지 (태그만 제거)
intrinsic_prompt: false # qm9 계열은 chemdfm 맞춤 이용

# Wrapper 설정
# wrapper: true → [Round 0]\nHuman: ...\nAssistant: 형식으로 감싸기
# wrapper: false → wrapper 없이 원본 instruction만 사용
wrapper: true
