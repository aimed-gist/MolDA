# GPT-4 API Configuration
# Used for OpenAI API-based benchmarking (no local GPU required)

# Model selection
# Options: gpt-4-0613, gpt-4-turbo, gpt-4o, gpt-4o-mini
llm_model: "gpt-4-0613"

# API-based model - freeze is the only option
tune_llm: freeze
peft_config: null
peft_dir: ""

# Not used for API models, but kept for compatibility
bert_hidden_dim: 768
bert_name: scibert
cross_attention_freq: 2
num_query_token: 0
bert_num_hidden_layers: 5
projector_type: qformer
load_in_8bit: false
lora_r: 64
lora_alpha: 32
lora_dropout: 0.1
selfies_token_path: model/selfies_dict.txt
add_selfies_tokens: false  # Not needed for GPT-4

# Generation parameters
num_beams: 1
gen_max_len: 256
min_len: 1
temperature: 0.0  # Deterministic for reproducibility
top_p: 1.0

# Request rate limiting (seconds between API calls)
request_delay: 0.1

# Trainer settings (minimal - API doesn't use GPU training)
strategy_name: null
accelerator: cpu  # API calls don't need GPU
devices: 1
precision: 32
max_steps: -1
max_epochs: 1
every_n_epochs: 1
task: null

llava_pretraining: 0
second_stage_start_epoch: 4
num_workers: 0
skip_sanity_check: true

# Batch settings - sequential API calls
accumulate_grad_batches: 1
total_batch_size: 1
batch_size: 1
inference_batch_size: 1  # API calls are sequential

truncation: 1
padding: max_length
max_length: 512
inference_max_length: 1024  # Larger for few-shot prompts

apply_sequence_packing: false
max_packing_size: -1

weight_decay: 0.0
min_lr: 0.0
init_lr: 0.0
warmup_lr: 0.0
warmup_epochs: 0
scheduler: linear_warmup_cosine_lr
optimizer: adamw
log_every_n_steps: 10
gradient_clip_val: 0.0

val_check_interval: 1.0
test_on_trainset: false

# Text only mode
mol_representation: string_only
log_attn_score: false
eval_modality_util: null
tune_gnn: false

# molpo
train_molpo: false
eval_molpo: false

find_unused_parameters: false

selfies_enumeration: false
isomericSmiles: false
canonical: false
allHsExplicit: false
